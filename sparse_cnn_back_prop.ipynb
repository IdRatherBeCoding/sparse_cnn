{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Custom Op Gradients in TensorFlow\n",
    "In my [forward prop implementation](https://github.com/IdRatherBeCoding/sparse_cnn/blob/master/sparse_cnn.ipynb) for sparse CNNs, I used [tf.py_func](https://www.tensorflow.org/api_docs/python/tf/py_func) to create a custom op to build $H_\\mathrm{out}$ and $Q$ from the sparse representation of the previous layer activations, $a^{[l-1]}$. The output activations are computed from Q using TensorFlow matmul and relu ($g$) ops:\n",
    "\n",
    "\\begin{equation*}\n",
    "a^{[l]} = g(Q(a^{[l-1]})\\cdot W + b).\n",
    "\\end{equation*}\n",
    "\n",
    "Since we are using TensorFlow ops to compute the matrix product and relu, TensorFlow will handle the derivatives for $g$ and the $Q.W$ product; we only have to implement the gradient of the custom py_func op itself. Specifically, given the gradient of the Loss with respect to our function's output, $\\frac{\\partial L}{\\partial Q}$, our gradient function needs to compute\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial a^{[l-1]}_{ij}} = \\sum_{pq} \\frac{\\partial L}{\\partial Q_{pq}} \\frac{\\partial Q_{pq}}{\\partial a^{[l-1]}_{ij}}.\n",
    "\\end{equation*}\n",
    "\n",
    "Recall that in place of the dense input activations $a^{[l]}$, we are using custom sparse representations: *SparseDataValue* and *SparseDataTensor*. These store the indices of the active sites, $H_\\mathrm{in}$, the values of the active sites, $M_\\mathrm{in}$, the dense shape and the ground state value for each channel. I will now introduce the notation $t_{\\mathrm{in},c}$ to represent the ground-state value of the $c^\\mathrm{th}$ channel.\n",
    "\n",
    "To enable back propagation, we will need to provide gradients with respect to $M_\\mathrm{in}$ and $t_\\mathrm{in}$. Dropping the subscript $\\mathrm{in}$ for clarity:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial M_{ij}} = \\sum_{pq} \\frac{\\partial L}{\\partial Q_{pq}} \\frac{\\partial Q_{pq}}{\\partial M_{ij}}\\\\\n",
    "\\frac{\\partial L}{\\partial t_{c}} = \\sum_{pq} \\frac{\\partial L}{\\partial Q_{pq}} \\frac{\\partial Q_{pq}}{\\partial t_{c}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradients of py_func ops\n",
    "I came across several discussions concerning this ([issue#1095](https://github.com/tensorflow/tensorflow/issues/1095), [SO1](https://datascience.stackexchange.com/questions/12974/tensorflow-how-to-set-gradient-of-an-external-process-py-func), [issue#3710](https://github.com/tensorflow/tensorflow/issues/3710), [SO2](https://stackoverflow.com/questions/38833934/write-custom-python-based-gradient-function-for-an-operation-without-c-imple)), but there doesn't appear to be an official guide specifically for py_func ops.\n",
    "\n",
    "The [adding an op](https://www.tensorflow.org/extend/adding_an_op#implement_the_gradient_in_python) guide describes how to register a gradient function using the [tf.RegisterGradient](https://www.tensorflow.org/api_docs/python/tf/RegisterGradient) decorator for an Op registered in C++. Unfortunately, RegisterGradient only registers functions to ops by type name. Since we're using py_func, the type of our custom op is always PyFunc. From the links above, there are two possible approaches: *Defun* and *gradient_override_map*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Defun approach\n",
    "Based on [this SO answer](https://stackoverflow.com/questions/38833934/write-custom-python-based-gradient-function-for-an-operation-without-c-imple). It it only [experimental](https://github.com/tensorflow/tensorflow/issues/14080) and [not ready for py_func](https://github.com/tensorflow/tensorflow/issues/10282), which I'll show below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Simple example: custom gradient for tf.square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def squared_back_prop(op, grad):\n",
    "    return tf.multiply(op.inputs[0] * 2.0, grad)\n",
    "\n",
    "@function.Defun(tf.float32, python_grad_func=squared_back_prop)\n",
    "def squared_forward_prop(a):\n",
    "    return tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.,  4.,  6.,  8.], dtype=float32)]\n",
      "error: 6.85453414917e-05\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(tf.constant(np.array([1., 2., 3., 4.]), dtype=tf.float32))\n",
    "x2 = squared_forward_prop(x)\n",
    "L = tf.reduce_sum(x2)\n",
    "dL = tf.gradients(L, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(dL))\n",
    "    print(\"error:\", tf.test.compute_gradient_error(x, [4], L, [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Defun example with py_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def square_numpy(x):\n",
    "    return np.square(x)\n",
    "\n",
    "@function.Defun(tf.float32, python_grad_func=squared_back_prop)\n",
    "def squared_forward_prop_py_func(a):\n",
    "    return tf.py_func(square_numpy, [a], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.,  4.,  6.,  8.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "x2 = squared_forward_prop_py_func(x)\n",
    "L = tf.reduce_sum(x2)\n",
    "dL = tf.gradients(L, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        print(sess.run(dL))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The gradient_override_map approach\n",
    "I will use the approach suggested in [issue#1095](https://github.com/tensorflow/tensorflow/issues/1095), and demonstrated in [this gist](https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342).\n",
    "\n",
    "A custom py_func function is defined, which takes a grad function. The grad function is given a random name and registered with tf.RegisterGradient.\n",
    "\n",
    "Finally, *gradient_override_map* is called before calling tf.py_func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# directly taken from https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342#gistcomment-2011084\n",
    "#\n",
    "# Define custom py_func which takes also a grad op as argument:\n",
    "def py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n",
    "    \n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "    \n",
    "    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "\n",
    "# Actual gradient:\n",
    "def _MySquareGrad(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    return grad * 2 * x  # add a \"small\" error just to see the difference:\n",
    "\n",
    "# Def custom square function using np.square instead of tf.square:\n",
    "def mysquare(x, name=None):\n",
    "    \n",
    "    with ops.name_scope(name, \"Mysquare\", [x]) as name:\n",
    "        sqr_x = py_func(np.square,\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        grad=_MySquareGrad)  # <-- here's the call to the gradient\n",
    "        return sqr_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.,  4.,  6.,  8.], dtype=float32)]\n",
      "error: 6.30617141724e-05\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(tf.constant(np.array([1., 2., 3., 4.]), dtype=tf.float32))\n",
    "x2 = mysquare(x)\n",
    "L = tf.reduce_sum(x2)\n",
    "dL = tf.gradients(L, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(dL))\n",
    "    print(\"error:\", tf.test.compute_gradient_error(x, [4], L, [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Great, that worked. Now let's try with a py_func op for the gradient too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _MyCubeGrad(op, grad):\n",
    "    name = \"MyCubeGrad\"\n",
    "    x = op.inputs[0]\n",
    "    cube_x_grad = py_func(lambda a: np.power(a, 2) * 3,\n",
    "                    [x],\n",
    "                    [tf.float32],\n",
    "                    name=name)\n",
    "    return cube_x_grad[0]\n",
    "\n",
    "def my_cube(x, name=None):\n",
    "    \n",
    "    with ops.name_scope(name, \"MyCube\", [x]) as name:\n",
    "        cube_x = py_func(lambda a: np.power(a, 3),\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        grad=_MyCubeGrad)\n",
    "        return cube_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  3.,  12.,  27.,  48.], dtype=float32)]\n",
      "error: 2.02655792236e-05\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(tf.constant(np.array([1., 2., 3., 4.]), dtype=tf.float32))\n",
    "x3 = my_cube(x)\n",
    "L = tf.reduce_sum(x3)\n",
    "dL = tf.gradients(L, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(dL))\n",
    "    print(\"error:\", tf.test.compute_gradient_error(x, [4], L, [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Ok, that's all good, now we can implement the gradient of Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compute $\\frac{\\partial L}{\\partial M_\\mathrm{in}}$\n",
    "We need to compute the derivative with respect to the active-site values, $M_\\mathrm{in}$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial M_{\\mathrm{in},ij}} = \\sum_{pq} \\frac{\\partial L}{\\partial Q_{pq}} \\frac{\\partial Q_{pq}}{\\partial M_{\\mathrm{in},ij}}.\n",
    "\\end{equation*}\n",
    "\n",
    "and the derivative with respect to the ground-state values, $t$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial t_{c}} = \\sum_{pq} \\frac{\\partial L}{\\partial Q_{pq}} \\frac{\\partial Q_{pq}}{\\partial t_{c}}\n",
    "\\end{equation*}\n",
    "\n",
    "Recall how Q is constructed: each row corresponds to an active site in the output; the values in each row correspond to the elements of $M_\\mathrm{in}$, ordered according to the filter weights to which their are visible. The derivative $\\frac{\\partial Q_{pq}}{\\partial M_{\\mathrm{in},ij}}$ is equal to 1 when the value $Q_{pq}$ was taken from $M_{\\mathrm{in},ij}$, otherwise it is equal to zero. So a given element $ij$ of the loss gradient is the sum of the $\\frac{\\partial L}{\\partial Q}$ elements for which $Q$ was assigned the value of $M_{\\mathrm{in},ij}$. A quick way to implement this is to take the loop structure used to build $Q$.\n",
    "\n",
    "For $\\frac{\\partial L}{\\partial t_{c}}$ we need to sum all the $\\frac{\\partial L}{\\partial Q_{pq}}$ values for which index $q$ corresponds to the ground state of channel $c$. To achieve this we will reshape Q from $(a_\\mathrm{out}, f^2n_\\mathrm{in})$ to $(a_\\mathrm{out}, f^2, n_\\mathrm{in})$ and initialize the output array as the sum over the first two axes. All we need to do next is subtract out the active-site values, which can be done at the same time as building $\\frac{\\partial L}{\\partial Q}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_Q(dLdQ, *inputs):\n",
    "    (H_in, M_in, dense_shape, f, n_in, ground_state) = inputs\n",
    "\n",
    "    dM = np.zeros_like(M_in)\n",
    "\n",
    "    height = dense_shape[0]\n",
    "    width = dense_shape[1]\n",
    "\n",
    "    output_sites = {}\n",
    "    # enumerate all output active sites and store the positions\n",
    "    # these could be reused from forward prop with a slight refactoring\n",
    "    i_out = 0\n",
    "    for [row, col] in H_in:\n",
    "        for i, j in filter_positions(row, col, height, width, f):\n",
    "            if (i, j) not in output_sites:\n",
    "                output_sites[(i, j)] = i_out\n",
    "                i_out += 1\n",
    "\n",
    "    a_out = i_out\n",
    "    # initialize dt by summing over all elements of dLdQ for each channel\n",
    "    dt = np.sum(dLdQ.reshape((a_out, f*f, n_in)), axis=(0, 1))\n",
    "    \n",
    "    for idx, [row, col] in enumerate(H_in):\n",
    "        # summing with explicit loops could be replaced by generating list of index permutations and summing slices\n",
    "        for i, j in filter_positions(row, col, height, width, f):\n",
    "            i_out = output_sites[(i, j)]\n",
    "            for i_val in range(n_in):\n",
    "                d = dLdQ[i_out, position_in_filter(i, j, row, col, f, i_val, n_in)]\n",
    "                dM[idx, i_val] += d\n",
    "                dt[i_val] -= d        \n",
    "       \n",
    "    return [\n",
    "        dM,\n",
    "        dt\n",
    "    ]\n",
    "\n",
    "def _grad_Q(op, *grads):\n",
    "    dM, dt = tf.py_func(grad_Q, [grads[1], *op.inputs], [op.inputs[1].dtype, op.inputs[5].dtype])\n",
    "    return [None, dM, None, None, None, dt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the modified py_func function to create the forward prop operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sparse_cnn_tensorflow.sparse_cnn import build_h_out_and_Q, next_ground_state, filter_positions, position_in_filter\n",
    "from sparse_cnn_tensorflow.sparse_data_tensor import SparseDataTensor\n",
    "\n",
    "def sparse_conv_2d(sparse_input, W, f, n_out, b):\n",
    "    H_in = sparse_input.H\n",
    "    M_in = sparse_input.M\n",
    "    dense_shape = sparse_input.dense_shape\n",
    "    n_in = dense_shape[2]\n",
    "    ground_state = sparse_input.ground_state\n",
    "\n",
    "    output_spatial_shape = (dense_shape[0] - f + 1, dense_shape[1] - f + 1)\n",
    "\n",
    "    H_out, Q = py_func(build_h_out_and_Q,\n",
    "                          [H_in, M_in, dense_shape, f, n_in, ground_state],\n",
    "                          [H_in.dtype, M_in.dtype], grad=_grad_Q)\n",
    "\n",
    "    M_out = tf.add(tf.matmul(Q, W), b)\n",
    "\n",
    "    output_dense_shape = (output_spatial_shape[0], output_spatial_shape[1], n_out)\n",
    "\n",
    "    output_ground_state = next_ground_state(W, ground_state, f) + b\n",
    "\n",
    "    return SparseDataTensor(H_out, M_out, output_dense_shape, output_ground_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error dM1/dM0: 1.89182003396e-13\n",
      "error dM1/dt1: 1.4199752485e-12\n",
      "error dM2/dM0: 1.18438592267e-12\n",
      "error dM2/dt0: 9.92272930489e-12\n",
      "error dt1/dt0: 5.50670620214e-14\n",
      "error dt2/dt0: 2.90967250294e-12\n"
     ]
    }
   ],
   "source": [
    "from sparse_cnn_tensorflow.sparse_data_tensor import SparseDataValue\n",
    "\n",
    "# using double precision to improve finite difference accuracy of tf.test.compute_gradient_error.\n",
    "\n",
    "f1 = 2\n",
    "n_in_1 = 2\n",
    "n_out_1 = 4\n",
    "\n",
    "W1 = tf.Variable(np.random.rand(f1*f1*n_in_1, n_out_1), dtype=tf.float64)\n",
    "b1 = tf.Variable(np.random.rand(n_out_1), dtype=tf.float64)\n",
    "\n",
    "f2 = 2\n",
    "n_in_2 = n_out_1\n",
    "n_out_2 = 8\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(f2*f2*n_in_2, n_out_2), dtype=tf.float64)\n",
    "b2 = tf.Variable(np.random.rand(n_out_2), dtype=tf.float64)\n",
    "\n",
    "x_dense = np.array([\n",
    "    [[1.7, 0.7], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],\n",
    "    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],\n",
    "    [[0.0, 0.0], [0.0, 0.0], [7.9, 0.9], [4.8, 0.8]],\n",
    "    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]\n",
    "], dtype=np.float64)\n",
    "\n",
    "x_sparse = SparseDataValue(x_dense)\n",
    "\n",
    "sparse_tensor = SparseDataTensor(\n",
    "        tf.constant(x_sparse.H),\n",
    "        tf.constant(x_sparse.M),\n",
    "        x_sparse.dense_shape,\n",
    "        tf.constant(x_sparse.ground_state))\n",
    "\n",
    "forward1 = sparse_conv_2d(sparse_tensor, W1, f1, n_out_1, b1)\n",
    "# gradient1 = tf.gradients(forward1.M, sparse_tensor.M)\n",
    "forward2 = sparse_conv_2d(forward1, W2, f2, n_out_2, b2)\n",
    "# gradient2 = tf.gradients(forward2.M, sparse_tensor.M)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print(sess.run(gradient[0]))\n",
    "    print(\"error dM1/dM0:\", tf.test.compute_gradient_error(sparse_tensor.M, [3, 2], forward1.M, [5, 4]))\n",
    "    print(\"error dM1/dt1:\", tf.test.compute_gradient_error(sparse_tensor.ground_state, [2], forward1.M, [5, 4]))\n",
    "    print(\"error dM2/dM0:\", tf.test.compute_gradient_error(sparse_tensor.M, [3, 2], forward2.M, [4, 8]))\n",
    "    print(\"error dM2/dt0:\", tf.test.compute_gradient_error(sparse_tensor.ground_state, [2], forward2.M, [4, 8]))\n",
    "    print(\"error dt1/dt0:\", tf.test.compute_gradient_error(sparse_tensor.ground_state, [2], forward1.ground_state, [4]))\n",
    "    print(\"error dt2/dt0:\", tf.test.compute_gradient_error(sparse_tensor.ground_state, [2], forward2.ground_state, [8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
