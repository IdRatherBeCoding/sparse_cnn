{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Custom Op Gradients in TensorFlow\n",
    "In my [forward prop implementation](https://github.com/IdRatherBeCoding/sparse_cnn/blob/master/sparse_cnn.ipynb) for sparse CNNs, I used [tf.py_func](https://www.tensorflow.org/api_docs/python/tf/py_func) to create a custom op to build $H_\\mathrm{out}$ and $Q$ from the sparse representation of the previous layer activations, $a^{[l-1]}$. The output activations are computed from Q using TensorFlow matmul and relu ($g$) ops:\n",
    "\n",
    "\\begin{equation*}\n",
    "a^{[l]} = g(Q(a^{[l-1]})\\cdot W + b).\n",
    "\\end{equation*}\n",
    "\n",
    "Since we are using TensorFlow ops to compute the matrix product and relu, TensorFlow will handle the derivatives for $g$ and the $Q.W$ product; we only have to implement the gradient of the custom py_func op itself. Specifically, given the gradient of the Loss with respect to our function's output, $\\frac{\\partial L}{\\partial Q}$, our gradient function needs to compute\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial a^{[l-1]}_{ij}} = \\sum_{pq} \\frac{\\partial L}{\\partial Q_{pq}} \\frac{\\partial Q_{pq}}{\\partial a^{[l-1]}_{ij}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradients of py_func ops\n",
    "I came across several discussions concerning this ([issue#1095](https://github.com/tensorflow/tensorflow/issues/1095), [SO1](https://datascience.stackexchange.com/questions/12974/tensorflow-how-to-set-gradient-of-an-external-process-py-func), [issue#3710](https://github.com/tensorflow/tensorflow/issues/3710), [SO2](https://stackoverflow.com/questions/38833934/write-custom-python-based-gradient-function-for-an-operation-without-c-imple)), but there doesn't appear to be an official guide specifically for py_func ops.\n",
    "\n",
    "The [adding an op](https://www.tensorflow.org/extend/adding_an_op#implement_the_gradient_in_python) guide describes how to register a gradient function using the [tf.RegisterGradient](https://www.tensorflow.org/api_docs/python/tf/RegisterGradient) decorator for an Op registered in C++. Unfortunately, RegisterGradient only registers functions to ops by type name. Since we're using py_func, the type of our custom op is always PyFunc. From the links above, there are two possible approaches: *Defun* and *gradient_override_map*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Defun approach\n",
    "Based on [this SO answer](https://stackoverflow.com/questions/38833934/write-custom-python-based-gradient-function-for-an-operation-without-c-imple). It it only [experimental](https://github.com/tensorflow/tensorflow/issues/14080) and [not ready for py_func](https://github.com/tensorflow/tensorflow/issues/10282), which I'll show below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Simple example: custom gradient for tf.square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_back_prop(op, grad):\n",
    "    return tf.multiply(op.inputs[0] * 2.0, grad)\n",
    "\n",
    "@function.Defun(tf.float32, python_grad_func=squared_back_prop)\n",
    "def squared_forward_prop(a):\n",
    "    return tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.,  4.,  6.,  8.], dtype=float32)]\n",
      "error: 2.15768814087e-05\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(tf.constant(np.array([1., 2., 3., 4.]), dtype=tf.float32))\n",
    "x2 = squared_forward_prop(x)\n",
    "L = tf.reduce_sum(x2)\n",
    "dL = tf.gradients(L, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(dL))\n",
    "    print(\"error:\", tf.test.compute_gradient_error(x, [4], L, [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defun example with py_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def square_numpy(x):\n",
    "    return np.square(x)\n",
    "\n",
    "@function.Defun(tf.float32, python_grad_func=squared_back_prop)\n",
    "def squared_forward_prop_py_func(a):\n",
    "    return tf.py_func(square_numpy, [a], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pyfunc_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, token, args)\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m\"\"\"Calls the registered function for `token` with args.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_funcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"callback %s is not found\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pyfunc_0'"
     ]
    }
   ],
   "source": [
    "x2 = squared_forward_prop_py_func(x)\n",
    "L = tf.reduce_sum(x2)\n",
    "dL = tf.gradients(L, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        print(sess.run(dL))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The gradient_override_map approach\n",
    "I will use the approach suggested in [issue#1095](https://github.com/tensorflow/tensorflow/issues/1095), and demonstrated in [this gist](https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342).\n",
    "\n",
    "A custom py_func function is defined, which takes a grad function. The grad function is given a random name and registered with tf.RegisterGradient.\n",
    "\n",
    "Finally, *gradient_override_map* is called before calling tf.py_func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# directly taken from https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342#gistcomment-2011084\n",
    "#\n",
    "# Define custom py_func which takes also a grad op as argument:\n",
    "def py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n",
    "    \n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "    \n",
    "    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "\n",
    "# Actual gradient:\n",
    "def _MySquareGrad(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    return grad * 2 * x  # add a \"small\" error just to see the difference:\n",
    "\n",
    "# Def custom square function using np.square instead of tf.square:\n",
    "def mysquare(x, name=None):\n",
    "    \n",
    "    with ops.name_scope(name, \"Mysquare\", [x]) as name:\n",
    "        sqr_x = py_func(np.square,\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        grad=_MySquareGrad)  # <-- here's the call to the gradient\n",
    "        return sqr_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.,  4.,  6.,  8.], dtype=float32)]\n",
      "error: 4.91291284561e-05\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(tf.constant(np.array([1., 2., 3., 4.]), dtype=tf.float32))\n",
    "x2 = mysquare(x)\n",
    "L = tf.reduce_sum(x2)\n",
    "dL = tf.gradients(L, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(dL))\n",
    "    print(\"error:\", tf.test.compute_gradient_error(x, [4], L, [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Great, that worked. Now let's try with a py_func op for the gradient too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actual gradient:\n",
    "def _MyCubeGrad(op, grad):\n",
    "    name = \"MyCubeGrad\"\n",
    "    x = op.inputs[0]\n",
    "    cube_x_grad = py_func(lambda a: np.power(a, 2) * 3,\n",
    "                    [x],\n",
    "                    [tf.float32],\n",
    "                    name=name,\n",
    "                    grad=_MyCubeGrad)\n",
    "    return cube_x_grad[0]\n",
    "\n",
    "# Def custom square function using np.square instead of tf.square:\n",
    "def my_cube(x, name=None):\n",
    "    \n",
    "    with ops.name_scope(name, \"MyCube\", [x]) as name:\n",
    "        cube_x = py_func(lambda a: np.power(a, 3),\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        grad=_MyCubeGrad)\n",
    "        return cube_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  3.,  12.,  27.,  48.], dtype=float32)]\n",
      "error: 5.36441802979e-06\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(tf.constant(np.array([1., 2., 3., 4.]), dtype=tf.float32))\n",
    "x3 = my_cube(x)\n",
    "L = tf.reduce_sum(x3)\n",
    "dL = tf.gradients(L, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(dL))\n",
    "    print(\"error:\", tf.test.compute_gradient_error(x, [4], L, [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ok, that's all good, now we can implement the gradeint of Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
